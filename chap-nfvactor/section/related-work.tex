\section{Related Work}
\label{sec:nfvactor-related-work}

Since the introduction of NFV \cite{nfv-white-paper}, a broad range of studies have been carried out, for bridging the gap between specialized hardware and network functions \cite{hwang2015netvm, Han:EECS-2015-155, martins2014clickos, 199352}, scaling and managing NFV systems \cite{gember2012stratos, palkar2015e2}, flow migration \cite{rajagopalan2013split, khalid2016paving, gember2015opennf}, NF replication \cite{rajagopalan2013pico, sherry2015rollback}, and traffic steering \cite{simplifying}. In these work, NF instances are typically created as software modules running on separate VMs or containers. \nfactor~customizes a runtime system to run flow actors and embeds service chain processing within the flow actor abstraction, to achieve transparent and highly efficient failure resilience guarantee.


Among the existing studies, StatelessNF \cite{201545} shares similar design goals with~\nfactor, \ie, to enable transparent system scalability and failure resilience. However, the methodology used by StatelessNF is orthogonal to that of~\nfactor: it stores flow states in a reliable database \cite{ongaro2011fast} to achieve failure resilience, while~\nfactor~exploits the actor model. Compared with StatelessNF,~\nfactor~can approach line-rate packet processing and does not rely on RDMA equipment.

Besides, the Click modular router \cite{kohler2000click} is the first work to introduce modular design for simplifying NF construction.~The module graph scheduler used by~\nfactor~is partially inherited from the scheduler of Click. However,~\nfactor~uses such a scheduler to speed up actor processing. Flurries \cite{zhang2016flurries} proposes fine-grained per-flow NF processing, by dynamically assigning a flow to a lightweight NF. Sharing some similarities, \nfactor~enables micro service chain processing of each flow in one actor, but focuses on providing transparent failure tolerance based on the actor model. OpenBox \cite{OpenBox} merges the processing logic of multiple VNFs, therefore improving the modularity and processing performance. Even though \nfactor~uses the traditional sequential service chain, we believe that the flexibility of actor model can help us adopt the idea of OpenBox in \nfactor, which we leave for future exploration. StateAlyzr \cite{khalid2016paving} uses static analysis to automate flow state extraction and simply human effort for enabling flow migration. However, enabling high-performance flow migration still requires a holistic design like~\nfactor.

%while \nfactor~focuses on mainstream service chain processing and leaves this to future work.

%Even though the service chain concept has been expanded to service graphs in E2 \cite{palkar2015e2} and OpenBox \cite{OpenBox}, \nfactor uses the traditional service chain to process a flow as it still represents the mainstream processing method \cite{hwang2015netvm, martins2014clickos}.

%Compared with OpenNF \cite{gember2015opennf}, the flow migration completion time is improved by a large margin. Compared with FTMB \cite{sherry2015rollback},~\nfactor~achieves similar replication throughput and recovery time, while exhibiting a more stable end-to-end delay.

%To implement flow migration, existing systems \cite{gember2015opennf}\cite{rajagopalan2013split} require direct modification of the core processing logic of NF software, and mostly rely on an SDN controller to carry out migration protocol, involving non-negligible overhead. %message passing overhead that lowers packet processing speed of the system.
%Finally, the migration process is fully controlled by a  centralized SDN controller, which may not be scalable if there are many NF instances that need flow migration service.
%\nfactor~overcomes these issues using novelly designed NF APIs and a largely distributed framework, where flow states are easily extractable and migration is achieved directly in-between runtimes with only 3 steps of request-response. % and clean separation between NF processing logic and resilience enabling functionalities, as well as a system design based on the distributed actor framework.
%The actors can be migrated by communicating among themselves without the coordination from a centralized controller. A fast virtual switch is designed to achieve the functionality of a dedicated SDN switch. Only 3 rounds of request-response are needed for achieving flow migration, based on the actor framework and the customized virtual switch, enabling fast flow migration and high packet processing throughput.

%Flow replication to provide failure tolerance usually involves check-pointing the entire NF process image, % (where the NF software is running),
%and replica creation using the image \cite{sherry2015rollback,rajagopalan2013pico}. Temporary pause of an NF process is typically needed \cite{sherry2015rollback}, resulting in packet losses.
%\nfactor~is able to checkpoint all states of a flow in a transparent fashion to minimize loss and delay, based on a clean separation between NF processing logic and flow state. % enables the actor to directly store all the flow states of the service chain and transmit the flow states at any time without interfering the normal execution of the NF
%and enable transparent replication of service chains.  %Existing work \cite{sherry2015rollback} rely on automated tools to extract important state variables for replicating, which relies on static program analysis technique and may not accurately extract all the important state variables if the NF program is complicated and uses a new architecture.

%A NFV system \cite{nfv-white-paper} typically consists of a controller and many VNF instances. Each VNF instance is a virtualized device running NF software. VNF instances are connected into service chains, implementing certain network services, \eg, access service. Packets of a network flow go through the NF instances in a service chain in order before reaching the destination.

%A VNF instance constantly polls a network interface card (NIC) for packets. Using traditional kernel network stack incurs high context switching overhead \cite{martins2014clickos} and greatly compromise the packet processing throughput. To speed things up, hypervisors usually map the memory holding packet buffers directly into the address space of the VNF instances with the help of Intel DPDK\cite{dpdk} or netmap \cite{netmap}. VNF instances then directly fetch packets from the mapped memory area, avoiding expensive context switches. Recent NFV systems \cite{palkar2015e2, Han:EECS-2015-155, sherry2015rollback, martins2014clickos, hwang2015netvm} are all built using similar techniques.


%Even though using DPDK and netmap to improve the performance of packet processing has become a new trend. Existing flow management systems are still using kernel networking stack to implement the communication channel. On contrary, NFActor completely abandons the kernel networking stack, by constructing a reliable transmission module using DPDK. Using this reliable transmission module does not incur any context switches, thereby boosting the message throughput to 6 million messages per second in our evalution.
