%while \nfactor~focuses on mainstream service chain processing and leaves this to future work.

%Even though the service chain concept has been expanded to service graphs in E2 \cite{palkar2015e2} and OpenBox \cite{OpenBox}, \nfactor uses the traditional service chain to process a flow as it still represents the mainstream processing method \cite{hwang2015netvm, martins2014clickos}.

%Compared with OpenNF \cite{gember2015opennf}, the flow migration completion time is improved by a large margin. Compared with FTMB \cite{sherry2015rollback},~\nfactor~achieves similar replication throughput and recovery time, while exhibiting a more stable end-to-end delay.

%To implement flow migration, existing systems \cite{gember2015opennf}\cite{rajagopalan2013split} require direct modification of the core processing logic of NF software, and mostly rely on an SDN controller to carry out migration protocol, involving non-negligible overhead. %message passing overhead that lowers packet processing speed of the system.
%Finally, the migration process is fully controlled by a  centralized SDN controller, which may not be scalable if there are many NF instances that need flow migration service.
%\nfactor~overcomes these issues using novelly designed NF APIs and a largely distributed framework, where flow states are easily extractable and migration is achieved directly in-between runtimes with only 3 steps of request-response. % and clean separation between NF processing logic and resilience enabling functionalities, as well as a system design based on the distributed actor framework.
%The actors can be migrated by communicating among themselves without the coordination from a centralized controller. A fast virtual switch is designed to achieve the functionality of a dedicated SDN switch. Only 3 rounds of request-response are needed for achieving flow migration, based on the actor framework and the customized virtual switch, enabling fast flow migration and high packet processing throughput.

%Flow replication to provide failure tolerance usually involves check-pointing the entire NF process image, % (where the NF software is running),
%and replica creation using the image \cite{sherry2015rollback,rajagopalan2013pico}. Temporary pause of an NF process is typically needed \cite{sherry2015rollback}, resulting in packet losses.
%\nfactor~is able to checkpoint all states of a flow in a transparent fashion to minimize loss and delay, based on a clean separation between NF processing logic and flow state. % enables the actor to directly store all the flow states of the service chain and transmit the flow states at any time without interfering the normal execution of the NF
%and enable transparent replication of service chains.  %Existing work \cite{sherry2015rollback} rely on automated tools to extract important state variables for replicating, which relies on static program analysis technique and may not accurately extract all the important state variables if the NF program is complicated and uses a new architecture.

%A NFV system \cite{nfv-white-paper} typically consists of a controller and many VNF instances. Each VNF instance is a virtualized device running NF software. VNF instances are connected into service chains, implementing certain network services, \eg, access service. Packets of a network flow go through the NF instances in a service chain in order before reaching the destination.

%A VNF instance constantly polls a network interface card (NIC) for packets. Using traditional kernel network stack incurs high context switching overhead \cite{martins2014clickos} and greatly compromise the packet processing throughput. To speed things up, hypervisors usually map the memory holding packet buffers directly into the address space of the VNF instances with the help of Intel DPDK\cite{dpdk} or netmap \cite{netmap}. VNF instances then directly fetch packets from the mapped memory area, avoiding expensive context switches. Recent NFV systems \cite{palkar2015e2, Han:EECS-2015-155, sherry2015rollback, martins2014clickos, hwang2015netvm} are all built using similar techniques.


%Even though using DPDK and netmap to improve the performance of packet processing has become a new trend. Existing flow management systems are still using kernel networking stack to implement the communication channel. On contrary, NFActor completely abandons the kernel networking stack, by constructing a reliable transmission module using DPDK. Using this reliable transmission module does not incur any context switches, thereby boosting the message throughput to 6 million messages per second in our evalution.
