\chapter {\nfactor: A Resilient NFV System using the Distributed Actor Model}
\label{ch:nfvactor}
\lhead{\chaptername \ \ref{ch:nfvactor}.\ \emph{\nfactor}}

This chapter discusses the design and implementation of \nfactor. The chapter starts with an introduction to \nfactor~using section \ref{sec:nfvactor-introduction}. Then the motivation for using the actor model is given in section \ref{sec:nfvactor-motivation}. The overall architecture of \nfactor~is given in \ref{sec:nfvactor-runtime-and-controller}, followed by the discussion of the APIs exposed by~\nfactor~for building NFs in section \ref{sec:nfvactor-nf-api}. The protocol for flow replication and migration is illustrated in section \ref{sec:nfvactor-migration-replication}. The performance of \nfactor~is demonstrated in section \ref{sec:nfvactor-evaluation}. Finally, a summary of \nfactor~is presented in section \ref{sec:nfvactor-conclusion}.

\section{Introduction}
\label{sec:nfvactor-introduction}

Network function virtualization (NFV) advocates moving
{\em network functions} (NFs) out of dedicated hardware middleboxes and running them as virtualized applications on commodity servers \cite{nfv-white-paper}. To enable effective large-scale deployment of virtual NFs, a number of NFV management systems have been proposed in recent years \cite{palkar2015e2, OpenBox, sekar2012design, anderson2012xomb, gember2012stratos, zhang2016opennetvm}, implementing a broad range of management functionalities. Among these functionalities, resilience guarantee, supported by flow migration and failure recovery mechanisms, is of particular importance in practical NFV systems.

% They implement a broad range of management functionalities and two of %A network service typically consists of a sequence of NFs for flow processing, described by a {\em service chain}, \eg, ``firewall $\rightarrow$ intrusion detection system (IDS) $\rightarrow$ loader balancer''.

{\em Resilience to failures \cite{sherry2015rollback,rajagopalan2013pico} is crucial for stateful NFs.}  Many NFs maintain important per-flow states \cite{EnablingNF}: IDSs such as Bro \cite{bro} store and update protocol-related states for each flow for issuing alerts for potential attacks; firewalls \cite{firewall} parse TCP SYN/ACK/FIN packets and maintain TCP connection related states for each flow; load balancers \cite{lvs} may retain mapping between a flow identifier and the server address, for modifying destination address of packets in the flow. It is critical to ensure correct recovery of flow states in case of NF failures, such that the connections handled by the failed NFs do not have to be reset -- a simple approach strongly rejected by middlebox vendors \cite{sherry2015rollback}.

{\em Efficient flow migration \cite{rajagopalan2013split, gember2015opennf, qazi2017high} is important for long-lived flows in case of dynamic system scaling.} Existing NFV systems \cite{palkar2015e2, gember2012stratos} mostly assume dispatching new flows to newly created NF instances when existing instances are overloaded, or waiting for remaining flows to complete before shutting down a mostly idle instance, which are efficient for short flows. Long flows are common on the Internet:  a web browser uses one TCP connection to exchange many requests and responses with a web server \cite{http-keep-alive}; video-streaming \cite{ffmpeg} and file-downloading \cite{ftp} systems maintain long-lived TCP connections for fetching large volumes of data from CDN servers. When NF instances handling such flows are overloaded or under-loaded, migrating flows to other available NF instances enables timely hotspot resolution or system cost minimization \cite{gember2015opennf}.

Even though failure resilience and efficient flow migration are important for NFV systems, enabling light-weight failure resilience and high-performance flow migration within existing NF software architecture has been a challenging task.

Failure resilience in the existing systems \cite{sherry2015rollback,rajagopalan2013pico} is typically implemented through checkpointing: each NF process is regularly checkpointed, and if it fails, the system replays important log traces collected since the latest checkpoint to recover the failed NF. Compared to the normal packet processing delay of an NF that lies within tens of microseconds, the process of checkpointing is heavyweight and can cause extra delay up to thousands of microseconds \cite{sherry2015rollback, rajagopalan2013pico}.

%Flow migration in existing systems is typically governed by centralized controllers \cite{rajagopalan2013split, gember2015opennf} with limited scalability. When an NFV system is scaled up to handle tens of running NFs \cite{palkar2015e2}, the centralized controller may become overloaded when monitoring concurrent flow migration among multiple pairs of NFs. In addition, such a centralized controller is typically an SDN controller, which installs expensive SDN switching rules to update flow routes during flow migration and compromises the overall migration performance. Finally, even though multiple messages have to be exchanged when executing the flow migration protocol, existing systems \cite{rajagopalan2013split, gember2015opennf} still rely on kernel network stack to deliver these messages, which prolongs the migration completion time due to the inefficiency of the kernel network stack \cite{netmap}.

Flow migration in existing systems \cite{rajagopalan2013split, gember2015opennf} is typically governed by a centralized controller. It fully monitors the migration process of each flow by installing SDN rule to update the route of the flow and exchanging messages with the NFs over inefficient kernel networking stack \cite{netmap}. However, a practical NFV system needs to manage tens of running NFs and handle hundreds of thousands of concurrent flows. To migrate these flows, the controller needs to sequentially execute the migration process of each flow, install a large number of SDN rules and exchange many migration protocol messages through inefficient communication channel, which may prolong the flow migration completion time and inhibit flow migration from serving as a practical NFV management task.

%Flow migration in existing systems \cite{rajagopalan2013split, gember2015opennf} is typically governed by a centralized controller, which installs SDN switching rule to update flow route during the migration of each flow. Even though multiple messages have to be exchanged when executing the migration protocol, existing systems \cite{rajagopalan2013split, gember2015opennf} still rely on kernel network stack to deliver these messages, which may increase the message passing time due to the inefficiency of the kernel network stack \cite{netmap}. As a practical NFV system usually needs to process tens of thousands of concurrent flows \cite{palkar2015e2}, migrating these flows requires installing and exchanging tens of thousands of SDN rules and migration messages over inefficient kernel networking stack, which may prolong the flow migration completion time and inhibit flow migration from serving as a practical NFV management task.

%    and installing These factors inhibit existing flow migration systems from timely migrating tens of thousands of concurrent flows.

% Besides, enabling flow migration with existing NF software is not trivial: flow migration requires important NF states to be correctly extracted and serialized for transmission. However, a separation between NF states and core processing logic is not enforced in the state-of-the-art NF implementation. OpenNF \cite{gember2015opennf} reports that several thousands lines of patch code must be added to the NF software, \eg, Bro IPS \cite{bro} and Squid caching proxy \cite{squid}, in order to extract and serialize flow states. %

Besides, enabling flow migration with existing NF software is not trivial: OpenNF \cite{gember2015opennf} reports that thousands lines of patch code must be added to existing NF software \cite{bro, squid} in order to extract and serialize flow states, communicate with the controller and control flow migration. This approach mixes the logic for controlling flow migration together with the core NF logic. To maintain and upgrade such an NF, the developer must well understand both the core NF logic and the complicated flow migration process, which adds additional burden on the developer. %and impedes future development of NFV technology.

% Flow migration requires important NF states must be correctly extracted and serialized for transmission. However, a separation between NF states and core processing logic is not enforced in the state-of-the-art NF implementation.  OpenNF \cite{gember2015opennf} reports that several thousands lines of patch code must be added to the NF software (Bro IPS \cite{bro} and Squid caching proxy \cite{squid}) in order to extract and serialize flow states, let alone the source code for controlling flow migration. What's more, as the logic for controlling flow migration is mixed together with core NF logic, it becomes

%This makes the logic that implements flow migration to be mixed together with the core NF logic, making it hard to upgrade and extend both logics.

% Extracting and serializing important NF states have been a

In this chapter, we present \nfactor, a new software framework for building NFV systems with high-performance flow migration and lightweight failure resilience. Unlike previous systems \cite{sherry2015rollback, rajagopalan2013pico, rajagopalan2013split, gember2015opennf} which augment existing NF software with resilience support, \nfactor~explores new research opportunities brought by a holistic approach: \nfactor~provides a general framework with built-in resilience support by exploiting the distributed actor model \cite{actor-wiki}, and exposes several easy-to-use APIs for implementing NFs. Internally, \nfactor~delegates the processing of each individual flow to an unique flow actor. The flow actors run in high-performance runtime systems, handle flow processing and ensure their own resilience in a largely decentralized fashion. \nfactor~brings three major benefits.

$\triangleright$ {\em Transparent resilience guarantee.} \nfactor~ensures that once the NFs are implemented with the provided APIs, failure resilience of the NFs is immediately guaranteed. \nfactor~decouples resilience logic from core NF logic by incorporating resilience operations within the framework and only exposing APIs for building NFs. Using the APIs, programmers are fully liberated from reasoning about details of resilience operations, but only focus on implementing the processing logic of NFs and handling simple interaction for synchronizing shared states of NFs during resilience operations. The exposed APIs also ensure a clean separation between the core processing logic and important NF states, facilitating resilience operations.

$\triangleright$ {\em Lightweight failure resilience.} With the actor abstraction and cleanly separated NF states, \nfactor~is able to replicate each flow independently without checkpointing the entire NF process. Each flow actor can replicate itself by constantly saving its per-flow state to another actor that serves as a replica.
%, which only involves sending single-trip messages.
This lightweight resilience operation guarantees good throughput, short recovery time and a small packet processing delay.

$\triangleright$ {\em High-performance flow migration.} The use of the actor model enables~\nfactor~to adopt a largely decentralized flow migration process: each flow actor can migrate itself by exchanging messages with other flow actors, while a centralized controller only initiates flow migration by instructing a runtime system about the amount of the flow actors that should be migrated. As a result,~\nfactor~is able to concurrently migrate a large number of flows among multiple pairs of runtime systems.~\nfactor~also replaces SDN switch with a lightweight virtual switch for flow redirection, simplifying flow redirection from updating SDN rule into modifying an runtime identifier number. %which eliminates the expensive operation of SDN rule updating.
The increased parallelism and simplified flow redirection jointly enhance the performance of flow migration.

Our major technical challenge is to build an actor runtime system to satisfy the stringent performance requirement of NFV application. Even the fastest actor runtime systems \cite{chs-rapc-16} may fail to deliver satisfactory packet processing performance due to their actor scheduling strategies and the use of kernel networking stack. To address this challenge, we carefully craft a high-performance actor runtime system by combining the performance benefits of (i) a module graph scheduler to effectively schedule multiple flow actors, (ii) a DPDK-based \cite{dpdk} fast packet I/O framework \cite{bess} to accelerate network packet processing and (iii) an efficient user-space message passing channel which completely bypasses the kernel network stack and improves the performance of both failure resilience and flow migration.

We implement \nfactor~and build several useful NFs using the exposed APIs. Our testbed experiments show that \nfactor~achieves 10Gbps line-rate processing for 64-byte packets, concurrent migration of 600K flows using around 700 milliseconds, and recovery of a single runtime within 70 milliseconds in case of failure. Compared with OpenNF \cite{gember2015opennf}, flow migration completion time in~\nfactor~can be 144 times faster. Compare with FTMB \cite{sherry2015rollback} for replication performance,~\nfactor~achieves similar packet processing throughput and recovery time, but with packet processing latency stabilized at around 20 microseconds. %The source code of \nfactor~is available at \cite{projectcode}.

%Going beyond resilience, a couple of interesting applications can also be efficiently enabled on \nfactor, including live NF update and correct MPTCP subflow processing. They require individual NFs to initiate flow migration, which is hard to achieve in the existing systems. The decentralized and fast flow migration in~\nfactor~enables these applications with ease.
