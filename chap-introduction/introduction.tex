\chapter {Introduction}
\label{ch:introduction}
\lhead{\chaptername \ \ref{ch:introduction}.\ \emph{Introduction}} %

\section {Network Function Virtualization}

Computer network \cite{Tanenbaum:2010:CN:1942194} is one of the most complicated systems that people have ever built. Except for all the end-points (mobile-phones, personal computers), switches and routers, a functional computer network also consists of many middleboxes, which are also referred to as network functions (NFs). While switches and routers serve as the core connection points in the computer network, NFs are crucial supplementary components that enrich the functionality of a computer network.

A firewall \cite{purdy2004linux} protects a network by preventing forbidden IP addresses from accessing the important service located inside the network. A network address translator (NAT) \cite{tsirtsis2000network} translates IP address between a public network and a private network, allowing end-points located inside a private network to communicate with other end-points located in the public network. An intrusion detection system (IDS) \cite{bro, paxson1999bro, snort} monitors the payload transmitted over an network connection and raises an alert if it detects malicious content. A virtual private network (VPN) gateway \cite{nobori2014vpn} encrypts client traffic to prevent client traffic from being intercepted and monitored. Apparently, without the supplementary functionalities provided by the NFs, the computer network would be easily harassed and attacked by malicious users, the communication between the private and public network would be problematic, and the privacy of the network communication would be compromised.

Despite of the importance of NFs, effective management of NFs has been notoriously hard for network operators \cite{sherry2012making}. NFs are usually implemented as proprietary hardware boxes, making them hard to deploy, maintain and upgrade. To deploy and maintain an NF, the network operator has to manually install the hardware box onto critical network path. To upgrade an NF, the network operator must spend extra labor to replace the old hardware box with a new one. As pointed out in \cite{sherry2012making}, handling hardware NFs has become a major trouble when managing computer networks.

Recently, with the quick development of cloud services \cite{ec2, azure, softlayer} and virtualization technology \cite{barham2003xen}, both industry and academia propose to replace hardware NFs with virtualized network functions (VNFs) running on standard virtualization platforms \cite{xen, kvm}. This trend is referred to as network function virtualization (NFV) \cite{nfv-white-paper}.

NFV can significantly reduce the effort and cost for deploying, maintaining and upgrading NFs. By leveraging the flexibility of virtulization technology, NFV makes it possible to dynamically scale virtualized NFs \cite{palkar2015e2}. The virtualized NFs are even capable of being resilient to workload fluctuation \cite{gember2012stratos} and machine failure \cite{sherry2015rollback}. NFV calls for continuous upgrade to the software architecture of the NFs, making NFs more robust under production environment \cite{201545}.

However, NFV technology is not born to be perfect. There are various challenging problems rooted within NFV technology, hindering the applicability of NFV. This thesis sets out to address three important problems existed among current NFV systems, covering the areas of scalability, resilience functionality and the programability of NFV.

%While NFV has drawn great attention from both industry and academia, there are still many unsolved problem associated with NFV technology, especially in the area of scalability when , resilience and simplifying the building high-performance asynchronous NFs.

%Howthere are still many unsolved problems associated with NFVs.

%To maintain and upgrade an NF, the network operator has to directly deal with the hardware box  these NFs, the network operator has to manually install these hardware boxes onto critical network path, configure them and maintain t

\section{Problems Tackled by This Thesis}

\subsection {Poor Scalability Across Geo-distributed Datacenters}

One of the key problems that NFV technology must tackle is to build an effective NFV management system that can dynamically scale VNF service chains -- an ordered collection of VNFs that altogether compose a network service, according to the traffic demand.  A number of management systems have also been proposed \cite{palkar2015e2, gember2012stratos}, which operate VNF service chains deployed in a single server cluster or datacenter. These management systems are adequate for service chains such as ``firewall$\rightarrow$ intrusion detection system (IDS)'', which are typically used to provide access service to a client-server web system, deployed in the on-premise cluster/datacenter of the service provider.

There are many other service chains providing communication services, e.g., the service chains in IP Multimedia Subsystems (IMS) \cite{3gpp-ims} and mobile core networks~\cite{epc}. These service chains enable people located in different areas to communicate with each other and their communication traffic must go through these service chains. Such service chains render a geo-distributed nature: the network functions are desirably deployed close to geo-dispersed users, and putting the service chain in a single datacenter would be unfavourable as compared to distributing its VNFs across several datacenters. By correctly placing VNFs of a service chain among different datacenters, the scaling system can reduce both the end-to-end delay and resource consumption.

However, the existing management systems have poor scalability when dealing with such geo-distributed service chains \cite{qazi2016klein}, due to the escalated challenges on efficient interconnection of VNFs over the WAN, dynamic decision making on how VNF instances are deployed in different datacenters, and optimally dispatching individual flows through the deployed instances.

\subsection{Inadequate Performance of Resilience Functionalities}

Besides scalability across geo-distributed datacenters, resilience functionalities, supported by flow migration and failure recovery mechanisms, are of particular importance in practical NFV systems.

{\em Resilience to failures \cite{sherry2015rollback,rajagopalan2013pico} is crucial for stateful NFs.}  Many NFs maintain important per-flow states \cite{EnablingNF}: IDSs such as Bro \cite{bro} store and update protocol-related states for each flow for issuing alerts for potential attacks; firewalls \cite{firewall} parse TCP SYN/ACK/FIN packets and maintain TCP connection related states for each flow; load balancers \cite{lvs} may retain mapping between a flow identifier and the server address, for modifying destination address of packets in the flow. It is critical to ensure correct recovery of flow states in case of NF failures, such that the connections handled by the failed NFs do not have to be reset -- a simple approach strongly rejected by middlebox vendors \cite{sherry2015rollback}.

{\em Efficient flow migration \cite{rajagopalan2013split, gember2015opennf, qazi2017high} is important for long-lived flows in case of dynamic system scaling.} Existing NFV systems \cite{palkar2015e2, gember2012stratos} mostly assume dispatching new flows to newly created NF instances when existing instances are overloaded, or waiting for remaining flows to complete before shutting down a mostly idle instance, which are efficient for short flows. Long flows are common on the Internet:  a web browser uses one TCP connection to exchange many requests and responses with a web server \cite{http-keep-alive}; video-streaming \cite{ffmpeg} and file-downloading \cite{ftp} systems maintain long-lived TCP connections for fetching large volumes of data from CDN servers. When NF instances handling such flows are overloaded or under-loaded, migrating flows to other available NF instances enables timely hotspot resolution or system cost minimization \cite{gember2015opennf}.

Even though failure resilience and efficient flow migration are important for NFV systems, enabling light-weight failure resilience and high-performance flow migration within existing NF software architecture has been a challenging task.

Failure resilience in the existing systems \cite{sherry2015rollback,rajagopalan2013pico} is typically implemented through checkpointing: each NF process is regularly checkpointed, and if it fails, the system replays important log traces collected since the latest checkpoint to recover the failed NF. Compared to the normal packet processing delay of an NF that lies within tens of microseconds, the process of checkpointing is heavyweight and can cause extra delay up to thousands of microseconds \cite{sherry2015rollback, rajagopalan2013pico}.

Flow migration in existing systems \cite{rajagopalan2013split, gember2015opennf} is typically governed by a centralized controller. It fully monitors the migration process of each flow by installing SDN rule to update the route of the flow and exchanging messages with the NFs over inefficient kernel networking stack \cite{netmap}. However, a practical NFV system needs to manage tens of running NFs and handle hundreds of thousands of concurrent flows. To migrate these flows, the controller needs to sequentially execute the migration process of each flow, install a large number of SDN rules and exchange many migration protocol messages through inefficient communication channel, which may prolong the flow migration completion time and inhibit flow migration from serving as a practical NFV management task.

Except for the inadequate performance, enabling flow migration with existing NF software is not trivial: OpenNF \cite{gember2015opennf} reports that thousands lines of patch code must be added to existing NF software \cite{bro, squid} in order to extract and serialize flow states, communicate with the controller and control flow migration. This approach mixes the logic for controlling flow migration together with the core NF logic. To maintain and upgrade such an NF, the developer must well understand both the core NF logic and the complicated flow migration process, which adds additional burden on the developer.

\subsection{High-performance NFs with Asynchronous Operations are Difficult to Build using Callbacks}

Network Functions (NFs) are more than simple packet processors that perform various transformations on each received packet. Modern NFs, \eg, firewall \cite{201545}, NAT \cite{201545}, IDS \cite{bro}, and proxies \cite{haproxy, project-clearwater}, often need to contact external services while processing network flows, \eg, for
%retrieving useful information from an external database, querying a DNS service
querying external databases~\cite{telephone-number-mapping, bro-scripting-tutorial}, or saving critical per-flow states on external reliable storage (for failure resilience purposes) \cite{201545}. To ensure high-speed packet processing while executing external queries, these NFs must fully exploit asynchronous programming: after generating a request to an external service, the NF should not block and wait for the response in a synchronous fashion; instead, it can save the current processing context and register an event handler function to handle the response upon its return, and switch to process other packets, potentially generating additional asynchronous requests.

%To implement efficient L4 flow processing, such an NF must fully utilize flow-level asynchrony: after processing a single packet in a flow \chuan{while no further upcoming packets are received in the flow?}, the NF should move on to process packets of other flows for xxx \chuan{give the purpose}, while retaining %without undermining
% the processing context of the original flow \chuan{explain what context is referring to}. Various NF software \cite{snort, 201546, haproxy} uses callback-based asynchronous programming to achieve flow-level asynchrony: after processing a packet, the NF saves context information of the current flow and switches to process other flows; when a new packet of the first flow arrives, the saved context is retrieved and a pre-determined callback function \chuan{not clear what `pre-determined' means and briefly describe what the callback function does} is invoked to serve the flow.


%Some modern NFs may need to contact external services, \eg, to be resilient to failures \cite{201545} \chuan{not clear why `to be resilient to failures' is relevant to `contact external services'}, collaborate with other NFs \cite{3gpp-ims} \chuan{be more concrete in this example by giving an collaboration example},  and retrieve useful information from a DNS server \cite{telephone-number-mapping, bro-scripting-tutorial}. For such NFs, besides flow-level asynchrony, request-level asynchrony is needed, that the NF switches to process the following packets in the same flow without waiting for handling of the previous ones (through the external services) to complete \chuan{improve my description of `request-level asynchrony': is it based on packets or some sort of `requests'?}.

%For example, to detect whether a file transmitted over a TCP connection contains a piece of malware, a Bro IDS \cite{bro} issues a DNS query containing the SHA1 hash \cite{sha1} of the file extracted from the reconstructed byte stream of the TCP flow to a Malware Hash Registry (MHR) \cite{MHR}. Then, the MHR generates a DNS response indicating whether the hash matches that of some known malware. To ensure high performance, the Bro IDS does not block and wait for the MHR response to arrive. Instead, it registers a callback function to handle the MHR response upon its receipt, in an asynchronous fashion, and switches to process other flows/packets or handle other generated events (new packet arrival, new reconstructed payload, etc. \cite{paxson1999bro}).
%\chuan{add two figures to illustrate flow-level asynchrony and request-level asynchrony in the above two paragraphs using two NF examples (switching among multiple flows, switching among packets in the same flow while accessing external devices).}

Compared with synchronous NF programs, asynchronous NF implementation using callbacks is significantly more efficient in packet processing, as it does not waste important CPU time. However, callback-based asynchronous programming has some inherent drawbacks that can escalate the difficulty for building NFs running asynchronous operations.
%, where asynchronous operations are often.

\textit{First,} compared to a synchronous program, a callback-based asynchronous program is harder to implement and reason about. %When a series of asynchronous operations are concatenated together,
Such a program may define multiple callback functions, scattered within different source files, to achieve a series of asynchronous operations. For example, the Bro IDS can be configured to detect malware in flows using two nested callback functions, a first callback function to handle the reply from a local database query which may trigger another callback function to handle the reply to a MHR query~ (Sec.~\ref{sec:bro}); and a NAT may replicate important per-flow states in an external database using 4 consecutive callbacks, to read from and write to a remote database while a TCP connection through the NAT is being established \cite{201545}. Dealing with multiple callbacks scattered in different source files can be confusing, and make it more difficult for a programmer to trace the execution order of the program. %, which may increase the possibility for introducing software bugs.

%disrupts the control flow of the program \cite{}: inside one callback function, the programmer may lose track of execution order of the program, making.
%On the other hand, concatenating a series of asynchronous operations can be important to modern NF design, as modern NFs can leverage this programming pattern to detect transmitted malwares (Sec.~\ref{sec:bro}) and replicate important per-flow state to recover from failure \cite{}.

%\textit{Second,} retrieving saved context information after registering a callback is a non-trivial job, which can be error-prone. Since an asynchronous program immediately switches to other tasks after registering a callback, the program must save the context before switching, and properly recover the context when the callback is eventually invoked. \chuan{explain more clearly what the context includes, such that readers can understand the following claim better} Failing to do so may lead to invalid memory access and program crash \cite{lu2008learning}. However, tracking context information is not easy, especially when the context is passed among multiple callback functions created due to a serious of asynchronous operations \chuan{explain more clearly why `tracking context information is not easy' or `retrieving saved context information after registering a callback is a non-trivial job'}.

\textit{Second,} visiting saved context information inside a registered callback can be error-prone. Since an asynchronous program immediately switches to other tasks after saving the context and registering a callback, the program must ensure that the saved context is not accidentally freed until the callback is invoked. Failing to do so may lead to invalid memory access and program crash. However, when multiple callback functions are used, the programmer may accidentally free the context if he fails to correctly trace the execution order of the callbacks.

\textit{Third,} redundant error handling code may be introduced in a callback-based asynchronous program. Since exceptions may happen when waiting for the external response, the program must properly handle the exceptions by either registering an error handling function or implementing exception handling logic in the callback registered to handle the response. When a series of asynchronous operations are executed, the programmer needs to add exception handling logic for each asynchronous operation. %failing to handle any may lead the program into incorrect state.
 Since the asynchronous operations/callback functions may well be scattered among multiple files, duplicate error handling code may need to be added in multiple places.
%It becomes tedious to redundantly add error handling code when multiple callback functions are defined

\section{Solutions Proposed by This Thesis}

Regarding the three empirical problems, this thesis proposes three NFV systems, namely \textit{ScalIMS}, \nfactor and \netstar, that are designed from ground-up to solve each of the three problems.

\subsection{\textit{ScalIMS}}

\textit{ScalIMS} is an NFV management system that enables dynamic deployment and scaling of VNF service chains across multiple datacenters, using representative control-plane and data-plane service chains of the IMS system \cite{3gpp-ims}. \textit{ScalIMS} is designed to provide good performance (minimal VNF instances deployment and guaranteed end-to-end flow delays), using both runtime statistics of VNFs and global traffic information. IMS is chosen as the target platform because of its important role in the telecom core networks as well as the accessibility of open-source software implementation of IMS \cite{project-clearwater}. \textit{ScalIMS} has two important characteristics that distinguish itself from existing management systems:

%\begin{itemize}
%\item
$\triangleright$ \textbf{Dynamic Scaling over Multiple Datacenters:} \textit{ScalIMS} dynamically deploys multiple instances of the same network function onto different datacenters according to real-time traffic demand and user distribution. The network paths that a service chain traverses are optimized to provide QoS guarantee of user traffic (i.e., bounded end-to-end delays). This feature distinguishes \textit{ScalIMS} from systems that can only scale service chains within a single datacenter \cite{palkar2015e2, gember2012stratos}.

%\item
$\triangleright$ \textbf{A Hybrid Scaling Strategy:} Most existing VNF management systems~\cite{wood2007black}~\cite{gember2012stratos} scale service chains using reactive approaches, adding/removing VNF instances by responding to changes of runtime status of existing VNFs. Novelly, \textit{ScalIMS} combines reactive scaling with proactive scaling, using predicted traffic volumes based on the history. This hybrid strategy exploits all opportunities for timely scaling of VNFs and significantly improves system performance.
%\end{itemize}

We evaluate \textit{ScalIMS} on IBM SoftLayer cloud. Experiment results show that \textit{ScalIMS} significantly improves QoS of user traffic compared with scaling systems that use only reactive or proactive scaling approaches. Meanwhile, \textit{ScalIMS} achieves this improvement %by either launching VNF instances timely or re-routing traffic to datacenters with redundant VNF instances,
 using almost $50\%$ less VNF instances. % When traffic re-routing takes effect, \textit{ScalIMS} achieves $30\%$ less VNF instance provisoining, while guaranteeing better QoS of user traffic. %the total number of provisioned VNF instances by at most $30\%$, when being compared with pure reactive based scaling approach.
%In this way, with \textit{ScalIMS}, service chains of an IMS can be deployed and scaled across multiple datacenters with QoS guarantee for user traffic and decreased operational cost of provisioning network function instances.
Even though \textit{ScalIMS} is designed for IMS systems, similar design principles can be easily applied to other NFV systems, which benefit from service chain deployment across multiple datacenters.

\subsection{\nfactor}

\nfactor is a new software framework for building NFV systems with high-performance flow migration and lightweight failure resilience. Unlike previous systems \cite{sherry2015rollback, rajagopalan2013pico, rajagopalan2013split, gember2015opennf} which augment existing NF software with resilience support, \nfactor~explores new research opportunities brought by a holistic approach: \nfactor~provides a general framework with built-in resilience support by exploiting the distributed actor model \cite{actor-wiki}, and exposes several easy-to-use APIs for implementing NFs. Internally, \nfactor~delegates the processing of each individual flow to an unique flow actor. The flow actors run in high-performance runtime systems, handle flow processing and ensure their own resilience in a largely decentralized fashion. \nfactor~brings three major benefits.

$\triangleright$ \textbf{Transparent resilience guarantee.} \nfactor~ensures that once the NFs are implemented with the provided APIs, failure resilience of the NFs is immediately guaranteed. \nfactor~decouples resilience logic from core NF logic by incorporating resilience operations within the framework and only exposing APIs for building NFs. Using the APIs, programmers are fully liberated from reasoning about details of resilience operations, but only focus on implementing the processing logic of NFs and handling simple interaction for synchronizing shared states of NFs during resilience operations. The exposed APIs also ensure a clean separation between the core processing logic and important NF states, facilitating resilience operations.

$\triangleright$ \textbf{Lightweight failure resilience.} With the actor abstraction and cleanly separated NF states, \nfactor~is able to replicate each flow independently without checkpointing the entire NF process. Each flow actor can replicate itself by constantly saving its per-flow state to another actor that serves as a replica.
%, which only involves sending single-trip messages.
This lightweight resilience operation guarantees good throughput, short recovery time and a small packet processing delay.

$\triangleright$ \textbf{High-performance flow migration.} The use of the actor model enables~\nfactor~to adopt a largely decentralized flow migration process: each flow actor can migrate itself by exchanging messages with other flow actors, while a centralized controller only initiates flow migration by instructing a runtime system about the amount of the flow actors that should be migrated. As a result,~\nfactor~is able to concurrently migrate a large number of flows among multiple pairs of runtime systems.~\nfactor~also replaces SDN switch with a lightweight virtual switch for flow redirection, simplifying flow redirection from updating SDN rule into modifying an runtime identifier number. %which eliminates the expensive operation of SDN rule updating.
The increased parallelism and simplified flow redirection jointly enhance the performance of flow migration.

Our major technical challenge is to build an actor runtime system to satisfy the stringent performance requirement of NFV application. Even the fastest actor runtime systems \cite{chs-rapc-16} may fail to deliver satisfactory packet processing performance due to their actor scheduling strategies and the use of kernel networking stack. To address this challenge, we carefully craft a high-performance actor runtime system by combining the performance benefits of (i) a module graph scheduler to effectively schedule multiple flow actors, (ii) a DPDK-based \cite{dpdk} fast packet I/O framework \cite{bess} to accelerate network packet processing and (iii) an efficient user-space message passing channel which completely bypasses the kernel network stack and improves the performance of both failure resilience and flow migration.

We implement \nfactor~and build several useful NFs using the exposed APIs. Our testbed experiments show that \nfactor~achieves 10Gbps line-rate processing for 64-byte packets, concurrent migration of 600K flows using around 700 milliseconds, and recovery of a single runtime within 70 milliseconds in case of failure. Compared with OpenNF \cite{gember2015opennf}, flow migration completion time in~\nfactor~can be 144 times faster. Compare with FTMB \cite{sherry2015rollback} for replication performance,~\nfactor~achieves similar packet processing throughput and recovery time, but with packet processing latency stabilized at around 20 microseconds. %The source code of \nfactor~is available at \cite{projectcode}.

%Going beyond resilience, a couple of interesting applications can also be efficiently enabled on \nfactor, including live NF update and correct MPTCP subflow processing. They require individual NFs to initiate flow migration, which is hard to achieve in the existing systems. The decentralized and fast flow migration in~\nfactor~enables these applications with ease.

\subsection{\netstar}

To counter the complexity associated with callback-based asynchronous programming, we borrow the power of future/promise paradigm \cite{li2007combining, claessen1999poor, wtf}, an advanced programming abstraction, and design \netstar, a programming framework for simple, elegant asynchronous programming in NFs.

%For the first time, \netstar~brings efficient future/promise abstraction to the dataplane software.
\netstar~enables programming a series of asynchronous operations (when processing dataplane packets) in a manner similar to implementing a simple synchronous program, while not incurring any performance degradation due to blocking as in a synchronous program.

% \chuan{The following two paragraphs on technical contributions are non-satisfactory: think hard and write again; you should explain more on how our design addresses the three callback problems listed above}

The power of \netstar~is mainly attributed to a programming interface that we design, the async-flow interface, which effectively combines network packet handling process with the future/promise abstraction. The async-flow interface is powered by a simulated packet processing loop, and uses the returned future objects from a packet handler function for implementing core NF processing logic. % to concatenate asynchronous operations.
 With this interface, programmers can simplify the implementation of complex asynchronous operations in NFs by chaining a series of continuation functions, which mimics a synchronous program. Due to the future/promise paradigm, programmers can avoid redundant error handling logic but use a set of consolidate error handling code, allowing them to focus more on the core NF processing logic. The async-flow interface also simplifies context management: a programmer only needs to keep track of a pointer to a context object, and subsequent visits to the context object is guaranteed to be safe.

To evaluate the performance of \netstar, we build a number of NFs using the framework, including four NFs from the StatelessNF paper \cite{201545}, an HTTP reverse proxy, an IDS and a malware detector. %...\chuan{update the list according to what you have}.
With extensive experiments, we show that NFs based on \netstar~use substantially fewer lines of code to implement asynchronous packet processing, as compared to callback-based implementation, while delivering sufficiently good performance in terms of packet processing throughput and latency. We also compare \netstar~with a coroutine based implementation, and show that the coroutine is a less desirable paradigm for implementing NFs processing a large number of concurrent network flows.

\subsection{A Summary of the Three Systems}

The three systems mentioned in previous sections introduce important guidelines for building scalable, resilient and high-performance NFV systems. Both \textit{ScalIMS} and~\nfactor~improve the scalability of NFV systems. \textit{ScalIMS} provides protocols and algorithms to scale service chains across multiple datacenters for improved traffic quality and decreased resource consumption. But when scaling service chains inside a single datacenter, \textit{ScalIMS} still relies on a primitive method which monitors the workload and passively provisions additional NF instances.~\nfactor~makes NFV scaling faster inside a single datacenter via high-performance flow migration, and it is a good candidate for replacing the primitive method used by \textit{ScalIMS}.~\nfactor~further provides high-performance failure resilience for NFV systems by replicating important flow states. By combining \textit{ScalIMS} and~\nfactor, the network operators can coordinate the scaling of service chains across multiple datacenters, boost the scaling performance inside a single datacenter and make the operations of NFV systems more reliable.

Aside from the basic packet processing task, critical components in both \textit{ScalIMS} and~\nfactor~communicate with each other to deliver important messages, notifications and events. To ensure high-performance, \textit{ScalIMS} and~\nfactor~rely on extensive asynchronous programming to avoid blocking during asynchronous communications and improve system throughput.~\netstar~becomes a suitable platform for refactoring \textit{ScalIMS} and~\nfactor. Using future-promise paradigm,~\netstar~can easily transform complicated asynchronous operations into simple chains of continuation functions and improve the robustness of \textit{ScalIMS} and~\nfactor. The high-performance nature of~\netstar~also guarantees that the refactored systems enjoy high-performance, sometimes even better than the non-refactored versions.


\section{Contributions of This Thesis}

In summary, this thesis makes the following contributions.

\begin{enumerate}
\item We design and implement a practical NFV management system, \textit{ScalIMS}, for scaling representative service chains in IMS system across geo-distributed datacenters. Our system is deployed in a public cloud and evaluates to improve traffic QoS and reduce resource consumption.
\item We introduce actor programming model for improving the parallelism of resilience functionality. To achieve the best performance when using actor model, we further design an efficient actor runtime system. It uses a novel module-graph scheduler for speeding up the packet processing and message passing performance.
\item We propose to use future/promise paradigm for simplifying asynchronous programming in NFs. To preserve the power of future/promise paradigm when processing network flows, we design async-flow interface that simulates a packet processing loop and achieves simplified asynchronous programming when handling network flows.
\end{enumerate}

\section{Thesis Organization}

The rest of the this thesis is organized as follows. Chapter \ref{ch:background} discusses the background and related work of this thesis. Chapter \ref{ch:scalims}, \ref{ch:nfvactor} and \ref{ch:netstar} discusses the design and implementation of \textit{ScalIMS}, \nfactor~and \netstar, respectively. A concluding remark and the future research directions of this thesis are given in chapter \ref{ch:conclusion}.
