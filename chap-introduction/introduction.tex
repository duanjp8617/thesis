\chapter {Introduction}
\label{ch:introduction}
\lhead{\chaptername \ \ref{ch:introduction}.\ \emph{Introduction}} %

\section{Introduction of \textit{ScalIMS}}

Traditional hardware-based network functions are notoriously hard and costly to deploy and scale. The recent paradigm of network function virtualization (NFV) advocates deploying software network functions in virtualized environments (e.g., VMs) on off-the-shelf servers, to significantly simplify deployment and scaling at much lowered costs~\cite{nfv-website}.

Despite the advantages, many problems remain when introducing NFV to the provisioning of practical network services. %, especially on dynamically scaling their service chains -- an ordered collection of virtual network functions (VNFs) that altogether compose a network service -- over a large geographical span.
 One problem is to design efficient VNF software, such that software VNFs can achieve packet processing speeds close to hardware middleboxes. Another is to design an efficient management system, which deploys and scales VNF service chains -- an ordered collection of VNFs that altogether compose a network service, according to the traffic demand. There have been efforts targeting architectural improvement of VNF software \cite{martins2014clickos}. A number of management systems have also been proposed \cite{palkar2015e2, gember2012stratos}, which operate VNF service chains deployed in a single server cluster or datacenter. These management systems are adequate for service chains such as ``firewall$\rightarrow$ intrusion detection system (IDS)'', %protect client-server based systems.
 which are typically used to provide access service to a client-server Web system, % and consisting of a firewall and an intrusion detection system (IDS) to access database and web services,
  deployed in the on-premise cluster/datacenter of the service provider.


  There are many other service chains which render a geo-distributed nature, e.g., the service chains in IP Multimedia Subsystems (IMS) \cite{3gpp-ims} and mobile core networks~\cite{epc}. In these systems, the network functions are desirably deployed close to geo-dispersed users, %However, they cannot be directly applied to other service chains that provide inter-connection services for users residing widely apart
 and putting the service chain in a single datacenter would be unfavourable as compared to distributing its VNFs across several datacenters. %, to enable low-delay access of the users to the network functions.
 The existing management systems cannot be directly applied to handle such geo-distributed service chains \cite{qazi2016klein}, due to the escalated challenges on efficient interconnection of VNFs over the WAN, dynamic decision making on how VNF instances are deployed in different datacenters, and optimally dispatching individual flows through the deployed instances.
 %and dynamic decision making on the numbers of VNF instances to deploy in different datacenters, adjusting them over time, and optimally dispatching individual flows through the deployed instances.



%Designing a management system for scaling geo-distributed service chains imposes new challenges over the existing approaches. How to provision VNF instances on different datacenters, how to adjust the provisioning over time with variation of traffic, and what VNF instances a service chain should go through at different datacenters are all important problems that must be well addressed.

This paper presents \textit{ScalIMS}, a management system that enables dynamic deployment and scaling of VNF service chains across multiple datacenters, using representative control-plane and data-plane service chains of the IMS system \cite{3gpp-ims}. \textit{ScalIMS} is designed to provide good performance (minimal VNF instances deployment and guaranteed end-to-end flow delays), using both runtime statistics of VNFs and global traffic information. IMS is chosen as the target platform because of its important role in the telecom core networks as well as the accessibility of open-source software implementation of IMS \cite{project-clearwater}. \textit{ScalIMS} has two important characteristics that distinguish itself from existing management systems:

%\begin{itemize}
%\item
$\triangleright$ \textbf{Dynamic Scaling over Multiple Datacenters:} \textit{ScalIMS} dynamically deploys multiple instances of the same network function onto different datacenters according to real-time traffic demand and user distribution. The network paths that a service chain traverses are optimized to provide QoS guarantee of user traffic (i.e., bounded end-to-end delays). This feature distinguishes \textit{ScalIMS} from systems that can only scale service chains within a single datacenter \cite{palkar2015e2, gember2012stratos}.

%\item
$\triangleright$ \textbf{A Hybrid Scaling Strategy:} Most existing VNF management systems~\cite{wood2007black}~\cite{gember2012stratos} scale service chains using reactive approaches, adding/removing VNF instances by responding to changes of runtime status of existing VNFs. Novelly, \textit{ScalIMS} combines reactive scaling with proactive scaling, using predicted traffic volumes based on the history. This hybrid strategy exploits all opportunities for timely scaling of VNFs and significantly improves system performance.
%\end{itemize}

We evaluate \textit{ScalIMS} on IBM SoftLayer cloud. Experiment results show that \textit{ScalIMS} significantly improves QoS of user traffic compared with scaling systems that use only reactive or proactive scaling approaches. Meanwhile, \textit{ScalIMS} achieves this improvement %by either launching VNF instances timely or re-routing traffic to datacenters with redundant VNF instances,
 using almost $50\%$ less VNF instances. % When traffic re-routing takes effect, \textit{ScalIMS} achieves $30\%$ less VNF instance provisoining, while guaranteeing better QoS of user traffic. %the total number of provisioned VNF instances by at most $30\%$, when being compared with pure reactive based scaling approach.
%In this way, with \textit{ScalIMS}, service chains of an IMS can be deployed and scaled across multiple datacenters with QoS guarantee for user traffic and decreased operational cost of provisioning network function instances.
Even though \textit{ScalIMS} is designed for IMS systems, similar design principles can be easily applied to other NFV systems, which benefit from service chain deployment across multiple datacenters.

\section{Introduction of \nfactor}

Network function virtualization (NFV) advocates moving
{\em network functions} (NFs) out of dedicated hardware middleboxes and running them as virtualized applications on commodity servers \cite{nfv-white-paper}. To enable effective large-scale deployment of virtual NFs, a number of NFV management systems have been proposed in recent years \cite{palkar2015e2, OpenBox, sekar2012design, anderson2012xomb, gember2012stratos, zhang2016opennetvm}, implementing a broad range of management functionalities. Among these functionalities, resilience guarantee, supported by flow migration and failure recovery mechanisms, is of particular importance in practical NFV systems.

% They implement a broad range of management functionalities and two of %A network service typically consists of a sequence of NFs for flow processing, described by a {\em service chain}, \eg, ``firewall $\rightarrow$ intrusion detection system (IDS) $\rightarrow$ loader balancer''.

{\em Resilience to failures \cite{sherry2015rollback,rajagopalan2013pico} is crucial for stateful NFs.}  Many NFs maintain important per-flow states \cite{EnablingNF}: IDSs such as Bro \cite{bro} store and update protocol-related states for each flow for issuing alerts for potential attacks; firewalls \cite{firewall} parse TCP SYN/ACK/FIN packets and maintain TCP connection related states for each flow; load balancers \cite{lvs} may retain mapping between a flow identifier and the server address, for modifying destination address of packets in the flow. It is critical to ensure correct recovery of flow states in case of NF failures, such that the connections handled by the failed NFs do not have to be reset -- a simple approach strongly rejected by middlebox vendors \cite{sherry2015rollback}.

{\em Efficient flow migration \cite{rajagopalan2013split, gember2015opennf, qazi2017high} is important for long-lived flows in case of dynamic system scaling.} Existing NFV systems \cite{palkar2015e2, gember2012stratos} mostly assume dispatching new flows to newly created NF instances when existing instances are overloaded, or waiting for remaining flows to complete before shutting down a mostly idle instance, which are efficient for short flows. Long flows are common on the Internet:  a web browser uses one TCP connection to exchange many requests and responses with a web server \cite{http-keep-alive}; video-streaming \cite{ffmpeg} and file-downloading \cite{ftp} systems maintain long-lived TCP connections for fetching large volumes of data from CDN servers. When NF instances handling such flows are overloaded or under-loaded, migrating flows to other available NF instances enables timely hotspot resolution or system cost minimization \cite{gember2015opennf}.

Even though failure resilience and efficient flow migration are important for NFV systems, enabling light-weight failure resilience and high-performance flow migration within existing NF software architecture has been a challenging task.

Failure resilience in the existing systems \cite{sherry2015rollback,rajagopalan2013pico} is typically implemented through checkpointing: each NF process is regularly checkpointed, and if it fails, the system replays important log traces collected since the latest checkpoint to recover the failed NF. Compared to the normal packet processing delay of an NF that lies within tens of microseconds, the process of checkpointing is heavyweight and can cause extra delay up to thousands of microseconds \cite{sherry2015rollback, rajagopalan2013pico}.

%Flow migration in existing systems is typically governed by centralized controllers \cite{rajagopalan2013split, gember2015opennf} with limited scalability. When an NFV system is scaled up to handle tens of running NFs \cite{palkar2015e2}, the centralized controller may become overloaded when monitoring concurrent flow migration among multiple pairs of NFs. In addition, such a centralized controller is typically an SDN controller, which installs expensive SDN switching rules to update flow routes during flow migration and compromises the overall migration performance. Finally, even though multiple messages have to be exchanged when executing the flow migration protocol, existing systems \cite{rajagopalan2013split, gember2015opennf} still rely on kernel network stack to deliver these messages, which prolongs the migration completion time due to the inefficiency of the kernel network stack \cite{netmap}.

Flow migration in existing systems \cite{rajagopalan2013split, gember2015opennf} is typically governed by a centralized controller. It fully monitors the migration process of each flow by installing SDN rule to update the route of the flow and exchanging messages with the NFs over inefficient kernel networking stack \cite{netmap}. However, a practical NFV system needs to manage tens of running NFs and handle tens of thousands of concurrent flows. To migrate these flows, the controller needs to sequentially execute the migration process of each flow, install a large number of SDN rules and exchange many migration protocol messages through inefficient communication channel, which may prolong the flow migration completion time and inhibit flow migration from serving as a practical NFV management task.

%Flow migration in existing systems \cite{rajagopalan2013split, gember2015opennf} is typically governed by a centralized controller, which installs SDN switching rule to update flow route during the migration of each flow. Even though multiple messages have to be exchanged when executing the migration protocol, existing systems \cite{rajagopalan2013split, gember2015opennf} still rely on kernel network stack to deliver these messages, which may increase the message passing time due to the inefficiency of the kernel network stack \cite{netmap}. As a practical NFV system usually needs to process tens of thousands of concurrent flows \cite{palkar2015e2}, migrating these flows requires installing and exchanging tens of thousands of SDN rules and migration messages over inefficient kernel networking stack, which may prolong the flow migration completion time and inhibit flow migration from serving as a practical NFV management task.

%    and installing These factors inhibit existing flow migration systems from timely migrating tens of thousands of concurrent flows.

% Besides, enabling flow migration with existing NF software is not trivial: flow migration requires important NF states to be correctly extracted and serialized for transmission. However, a separation between NF states and core processing logic is not enforced in the state-of-the-art NF implementation. OpenNF \cite{gember2015opennf} reports that several thousands lines of patch code must be added to the NF software, \eg, Bro IPS \cite{bro} and Squid caching proxy \cite{squid}, in order to extract and serialize flow states. %

Besides, enabling flow migration with existing NF software is not trivial: OpenNF \cite{gember2015opennf} reports that thousands lines of patch code must be added to existing NF software \cite{bro, squid} in order to extract and serialize flow states, communicate with the controller and control flow migration. This approach mixes the logic for controlling flow migration together with the core NF logic. To maintain and upgrade such an NF, the developer must well understand both the core NF logic and the complicated flow migration process, which adds additional burden on the developer. %and impedes future development of NFV technology.

% Flow migration requires important NF states must be correctly extracted and serialized for transmission. However, a separation between NF states and core processing logic is not enforced in the state-of-the-art NF implementation.  OpenNF \cite{gember2015opennf} reports that several thousands lines of patch code must be added to the NF software (Bro IPS \cite{bro} and Squid caching proxy \cite{squid}) in order to extract and serialize flow states, let alone the source code for controlling flow migration. What's more, as the logic for controlling flow migration is mixed together with core NF logic, it becomes

%This makes the logic that implements flow migration to be mixed together with the core NF logic, making it hard to upgrade and extend both logics.

% Extracting and serializing important NF states have been a

In this paper, we present \nfactor, a new software framework for building NFV systems with high-performance flow migration and lightweight failure resilience. Unlike previous systems \cite{sherry2015rollback, rajagopalan2013pico, rajagopalan2013split, gember2015opennf} which augment existing NF software with resilience support, \nfactor~explores new research opportunities brought by a holistic approach: \nfactor~provides a general framework with built-in resilience support by exploiting the distributed actor model \cite{actor-wiki}, and exposes several easy-to-use APIs for implementing NFs. Internally, \nfactor~delegates the processing of each individual flow to an unique flow actor. The flow actors run in high-performance runtime systems, handle flow processing and ensure their own resilience in a largely decentralized fashion. \nfactor~brings three major benefits.

$\triangleright$ {\em Transparent resilience guarantee.} \nfactor~ensures that once the NFs are implemented with the provided APIs, failure resilience of the NFs is immediately guaranteed. \nfactor~decouples resilience logic from core NF logic by incorporating resilience operations within the framework and only exposing APIs for building NFs. Using the APIs, programmers are fully liberated from reasoning about details of resilience operations, but only focus on implementing the processing logic of NFs and handling simple interaction for synchronizing shared states of NFs during resilience operations. The exposed APIs also ensure a clean separation between the core processing logic and important NF states, facilitating resilience operations.

$\triangleright$ {\em Lightweight failure resilience.} With the actor abstraction and cleanly separated NF states, \nfactor~is able to replicate each flow independently without checkpointing the entire NF process. Each flow actor can replicate itself by constantly saving its per-flow state to another actor that serves as a replica.
%, which only involves sending single-trip messages.
This lightweight resilience operation guarantees good throughput, short recovery time and a small packet processing delay.

$\triangleright$ {\em High-performance flow migration.} The use of the actor model enables~\nfactor~to adopt a largely decentralized flow migration process: each flow actor can migrate itself by exchanging messages with other flow actors, while a centralized controller only initiates flow migration by instructing a runtime system about the amount of the flow actors that should be migrated. As a result,~\nfactor~is able to concurrently migrate a large number of flows among multiple pairs of runtime systems.~\nfactor~also replaces SDN switch with a lightweight virtual switch for flow redirection, simplifying flow redirection from updating SDN rule into modifying an runtime identifier number. %which eliminates the expensive operation of SDN rule updating.
The increased parallelism and simplified flow redirection jointly enhance the performance of flow migration.

Our major technical challenge is to build an actor runtime system to satisfy the stringent performance requirement of NFV application. Even the fastest actor runtime systems \cite{chs-rapc-16} may fail to deliver satisfactory packet processing performance due to their actor scheduling strategies and the use of kernel networking stack. To address this challenge, we carefully craft a high-performance actor runtime system by combining the performance benefits of (i) a module graph scheduler to effectively schedule multiple flow actors, (ii) a DPDK-based \cite{dpdk} fast packet I/O framework \cite{bess} to accelerate network packet processing and (iii) an efficient user-space message passing channel which completely bypasses the kernel network stack and improves the performance of both failure resilience and flow migration.

We implement \nfactor~and build several useful NFs using the exposed APIs. Our testbed experiments show that \nfactor~achieves 10Gbps line-rate processing for 64-byte packets, concurrent migration of 600K flows using around 700 milliseconds, and recovery of a single runtime within 70 milliseconds in case of failure. Compared with OpenNF \cite{gember2015opennf}, flow migration completion time in~\nfactor~can be 144 times faster. Compare with FTMB \cite{sherry2015rollback} for replication performance,~\nfactor~achieves similar packet processing throughput and recovery time, but with packet processing latency stabilized at around 20 microseconds. The source code of \nfactor~is available at \cite{projectcode}.

%Going beyond resilience, a couple of interesting applications can also be efficiently enabled on \nfactor, including live NF update and correct MPTCP subflow processing. They require individual NFs to initiate flow migration, which is hard to achieve in the existing systems. The decentralized and fast flow migration in~\nfactor~enables these applications with ease.

\section{Introduction of \netstar}

Network Functions (NFs) are more than simple packet processors that perform various transformations on each received packet. Modern NFs, \eg, firewall \cite{201545}, NAT \cite{201545}, IDS \cite{bro}, and proxies \cite{haproxy, project-clearwater}, often need to contact external services while processing network flows, \eg, for
%retrieving useful information from an external database, querying a DNS service
querying external databases~\cite{telephone-number-mapping, bro-scripting-tutorial}, or saving critical per-flow states on external reliable storage (for failure resilience purposes) \cite{201545}. To ensure high-speed packet processing while executing external queries, these NFs must fully exploit asynchronous programming: after generating a request to an external service, the NF should not block and wait for the response in a synchronous fashion; instead, it can save the current processing context and register an event handler function to handle the response upon its return, and switch to process other packets, potentially generating additional asynchronous requests.

%To implement efficient L4 flow processing, such an NF must fully utilize flow-level asynchrony: after processing a single packet in a flow \chuan{while no further upcoming packets are received in the flow?}, the NF should move on to process packets of other flows for xxx \chuan{give the purpose}, while retaining %without undermining
% the processing context of the original flow \chuan{explain what context is referring to}. Various NF software \cite{snort, 201546, haproxy} uses callback-based asynchronous programming to achieve flow-level asynchrony: after processing a packet, the NF saves context information of the current flow and switches to process other flows; when a new packet of the first flow arrives, the saved context is retrieved and a pre-determined callback function \chuan{not clear what `pre-determined' means and briefly describe what the callback function does} is invoked to serve the flow.


%Some modern NFs may need to contact external services, \eg, to be resilient to failures \cite{201545} \chuan{not clear why `to be resilient to failures' is relevant to `contact external services'}, collaborate with other NFs \cite{3gpp-ims} \chuan{be more concrete in this example by giving an collaboration example},  and retrieve useful information from a DNS server \cite{telephone-number-mapping, bro-scripting-tutorial}. For such NFs, besides flow-level asynchrony, request-level asynchrony is needed, that the NF switches to process the following packets in the same flow without waiting for handling of the previous ones (through the external services) to complete \chuan{improve my description of `request-level asynchrony': is it based on packets or some sort of `requests'?}.

For example, to detect whether a file transmitted over a TCP connection contains a piece of malware, a Bro IDS \cite{bro} issues a DNS query containing the SHA1 hash \cite{sha1} of the file extracted from the reconstructed byte stream of the TCP flow to a Malware Hash Registry (MHR) \cite{MHR}. Then, the MHR generates a DNS response indicating whether the hash matches that of some known malware. To ensure high performance, the Bro IDS does not block and wait for the MHR response to arrive. Instead, it registers a callback function to handle the MHR response upon its receipt, in an asynchronous fashion, and switches to process other flows/packets or handle other generated events (new packet arrival, new reconstructed payload, etc. \cite{paxson1999bro}).
%\chuan{add two figures to illustrate flow-level asynchrony and request-level asynchrony in the above two paragraphs using two NF examples (switching among multiple flows, switching among packets in the same flow while accessing external devices).}

Compared with synchronous NF programs, asynchronous NF implementation using callbacks is significantly more efficient in packet processing, as it does not waste important CPU time. However, callback-based asynchronous programming has some inherent drawbacks that can prevent developers from building NFs with richer functionalities.
%, where asynchronous operations are often.

\textit{First,} compared to a synchronous program, a callback-based asynchronous program is harder to implement and reason about. %When a series of asynchronous operations are concatenated together,
Such a program may define multiple callback functions, scattered within different source files, to achieve a series of asynchronous operations. For example, the Bro IDS can be configured to detect malware in flows using two nested callback functions, a first callback function to handle the reply from a local database query which may trigger another callback function to handle the reply to a MHR query~ (Sec.~\ref{sec:bro}); and a NAT may replicate important per-flow states in an external database using 4 consecutive callbacks, to read from and write to a remote database while a TCP connection through the NAT is being established \cite{201545}. Dealing with multiple callbacks scattered in different source files can be confusing, and make it more difficult for a programmer to trace the execution order of the program. %, which may increase the possibility for introducing software bugs.

%disrupts the control flow of the program \cite{}: inside one callback function, the programmer may lose track of execution order of the program, making.
%On the other hand, concatenating a series of asynchronous operations can be important to modern NF design, as modern NFs can leverage this programming pattern to detect transmitted malwares (Sec.~\ref{sec:bro}) and replicate important per-flow state to recover from failure \cite{}.

%\textit{Second,} retrieving saved context information after registering a callback is a non-trivial job, which can be error-prone. Since an asynchronous program immediately switches to other tasks after registering a callback, the program must save the context before switching, and properly recover the context when the callback is eventually invoked. \chuan{explain more clearly what the context includes, such that readers can understand the following claim better} Failing to do so may lead to invalid memory access and program crash \cite{lu2008learning}. However, tracking context information is not easy, especially when the context is passed among multiple callback functions created due to a serious of asynchronous operations \chuan{explain more clearly why `tracking context information is not easy' or `retrieving saved context information after registering a callback is a non-trivial job'}.

\textit{Second,} visiting saved context information inside a registered callback can be error-prone. Since an asynchronous program immediately switches to other tasks after saving the context and registering a callback, the program must ensure that the saved context is not accidentally freed until the callback is invoked. Failing to do so may lead to invalid memory access and program crash. However, when multiple callback functions are used, the programmer may accidentally free the context if he fails to correctly trace the execution order of the callbacks.

\textit{Third,} redundant error handling code may be introduced in a callback-based asynchronous program. Since exceptions may happen when waiting for the external response, the program must properly handle the exceptions by either registering an error handling function or implementing exception handling logic in the callback registered to handle the response. When a series of asynchronous operations are executed, the programmer needs to add exception handling logic for each asynchronous operation. %failing to handle any may lead the program into incorrect state.
 Since the asynchronous operations/callback functions may well be scattered among multiple files, duplicate error handling code may need to be added in multiple places.
%It becomes tedious to redundantly add error handling code when multiple callback functions are defined

% First, the packet access pattern of a NF sometimes requires multiple asynchronous operations to be chained together in order to process a single packet \cite{201545}. This requires defining multiple callback functions and saving multiple contexts, which may significantly increase the number of the lines of code used for implementing simple NF logic. Second, due to the use of multiple callback functions, the control flow of the program is disrupted, making it hard to write and reason about the correctness of the resulting NF logic. Third, exception handling in existing event-driven framework can be repetitive, as each callback function needs to carefully handle possible error conditions. Finally, most existing event-driven programming framework is based on C programming language, which does not expose a safe programming interface. When the callback function is invoked, the callback function can literally modify arbitrary program state, which may crash the entire NF program if not carefully programmed.

%\chuan{clarify why using a more advanced programming abstraction is a natural solution: have people been aware of the above problems with callbacks and used coroutine or future/promise as the solution in other domains? If so, you should explain so, give examples and add citations.}

People have recognized the problems with callbacks when building event-driven systems such as web browsers \cite{gallaba2015don, kambona2013evaluation}, programming language runtime systems \cite{syme2011f}, web servers \cite{tornado-web-server} and database servers \cite{rethinkdb}. Their solution to counter the problems is to use a more advanced programming abstraction, such as the coroutine \cite{coroutine} and the future/promise paradigm
\cite{li2007combining, claessen1999poor, wtf}. Coroutine is a user-space cooperative thread that is able to execute asynchronous program in a fully synchronous fashion. %making asynchronous code easy to implement compared with callbacks.
 However, the coroutine switching time may cause and suffer considerable overhead in NFs processing a large number of concurrent network flows.
In contrast, the future/promise paradigm uses special runtime objects, futures, promises and continuation functions, to mimic synchronous programming while being fully asynchronous. Besides making asynchronous program easier to implement, the future/promise abstraction can also reduce redundant error handling code by effectively propagating exceptions to a consolidate error handling logic.% reduce redundant error handling code.  \chuan{explain more why future/promise could be a better solution to resolve the three problems with callbacks}.


Though the future/promise paradigm is promising, the future/promise abstraction had only existed in high-level programming languages such as F\# \cite{syme2011f}, Haskell \cite{li2007combining} and OCaml \cite{wtf}. It is known that high-level programming languages are not suitable for developing NF software due to their lower runtime efficiency as compared to C/C++-based implementation, and unpredictable processing delay caused by their garbage collectors \cite{199352}. A recent open-source C++ library, Seastar \cite{seastar}, is implementing the future/promise paradigm to build high-performance database servers \cite{scylladb}. The library is integrated with DPDK and provides a customized user-space TCP/IP stack for high-speed database queries. However, it does not expose any interface to manipulate raw network packets, and designing such an interface, which effectively processes network packets without diminishing the power of the future/promise abstraction, is non-trivial. A straightforward design may directly expose a regular packet handler function, that is invoked for each received packet. However, such a design falls back to callback-based asynchronous programming and leaves no room for utilizing the future/promise abstraction.
%, as a naive implementation may degrade the future/promise abstraction into regular callback functions. \chuan{clarify why `a naive implementation may degrade the future/promise abstraction into regular callback functions'.}

This paper proposes \netstar, a future/promise-based programming framework for simple, elegant asynchronous programming in NFs. %For the first time, \netstar~brings efficient future/promise abstraction to the dataplane software.
\netstar~enables programming a series of asynchronous operations (when processing dataplane packets) in a manner similar to implementing a simple synchronous program, while not incurring any performance degradation due to blocking as in a synchronous program.

% \chuan{The following two paragraphs on technical contributions are non-satisfactory: think hard and write again; you should explain more on how our design addresses the three callback problems listed above}

The power of \netstar~is mainly attributed to a programming interface that we design, the async-flow interface, which effectively combines network packet handling process with the future/promise abstraction. The async-flow interface is powered by a simulated packet processing loop, and uses the returned future objects from a packet handler function for implementing core NF processing logic. % to concatenate asynchronous operations.
 With this interface, programmers can simplify the implementation of complex asynchronous operations in NFs by chaining a series of continuation functions, which mimics a synchronous program. Due to the future/promise paradigm, programmers can avoid redundant error handling logic but use a set of consolidate error handling code, allowing them to focus more on the core NF processing logic. The async-flow interface also simplifies context management: a programmer only needs to keep track of a pointer to a context object, and subsequent visits to the context object is guaranteed to be safe.


% is used to implement complex asynchronous operations when processing raw packets and build a wide range of real-world NF applications. Instead, async-flow interface simulates a regular packet processing loop which is sequentially invoked to process each flow packet. Within the simulated loop, NF application can create arbitrarily complex asynchronous operation chains using future/promise abstraction to handle the flow packet.

%\textit{Seastar Library.} We patch Seastar to expose an interface for receiving and sending raw network packets, paving the way for building various NFs. In addition, we eliminate an extra packet copy by replacing the default Seastar packet object with a fast packet representation, which improves the raw packet processing throughput significantly.


%to effectively leverage future/promise abstraction to process dataplane packets, we have designed async-flow interface. The interface captures a wide range of real-world NF requirements. Using this interface, the NF program can execute complicated asynchronous operations with ease.

% In this paper, we proposed NetStar framework, which is designed to improve asynchronous programming in NF software. In particular, NetStar handles asynchronous operations of middleboxes in a way that is both efficient and manageable. Asynchronous operations in NetStar are accomplished through through callbacks, making NetStar highly efficient. However, the callbacks in NetStar are used in an implicit way that mimics the style of synchronous operations, making them easy to program with and reason about. NetStar's power comes from the promise-continuation programming model provided by Seastar \cite{seastar} and advanced C++14 features, such as lambda expression and move semantics.\chuan{use one sentence to explain what is seastar}

%Based on the advanced programming model, we design a general purpose asynchronous flow abstraction that can precisely capture a wide rage of real-world NF requirements. Using the asynchronous flow abstraction, the processing task of the flow can pause at any time to perform asynchronous operations and resume normal processing when the asynchronous operation finishes. Our model is memory safe and only exposes events that the user registers.

To evaluate the performance of \netstar, we build a number of NFs using the framework, including four NFs from the StatelessNF paper \cite{201545}, an HTTP reverse proxy, an IDS and a malware detector. %...\chuan{update the list according to what you have}.
With extensive experiments, we show that NFs based on \netstar~use substantially fewer lines of code to implement asynchronous packet processing, as compared to callback-based implementation, while delivering sufficiently good performance in terms of packet processing throughput and latency. We also compare \netstar~with a coroutine based implementation, and show that the coroutine is a less desirable paradigm for implementing NFs processing a large number of concurrent network flows. The source code of~\netstar~is available at \cite{netstar-source}.
%Moreover, we quantitatively evaluate the number of lines code of NFs implemented using \netstar~with NFs implemented using traditional call-backed asynchronous programming method. Our quantitative study shows that for NFs that need to perform asynchronous operations, \netstar~can effectively reduce the lines of code needed for implementing the core logic NF, by up to more than 20\%.

%\chuan{the following contributions are lame so remove} In summary, this paper makes the following contributions. (i) We are the first to introduce future/promise abstraction into dataplane software for handling complex asynchronous interactions, which can be important to modern NFs with varying real-world needs. chuan: but SeaStar can achieve it already (ii) We have designed and implemented async-flow interface, which exposes the full power of future/promise abstraction and enable NF applications to implement complex asynchronous operations. (iii) We implement several NFs that are of important industrial and research values using NetStar, verify they can achieve good performance while enjoy a simplified implementation.

% We make the following contribution in this paper:

% First, we extend Seastar into a new framework for efficiently building asynchronous NFs. Second, we define a general purpose asynchronous flow model that can capture the requirement of a wide-range of real-world middleboxes. Finally, we use NetStar to build some practical middleboxes and quantitatively evaluate its effectiveness in reducing the lines of implementation code.
